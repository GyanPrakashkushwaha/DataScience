{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCNVAaacXfWuQdI1Qdu60+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fast RCNN\n","\n","## Intuition of Fast RCNN\n","What else can we do to reduce the computation time an RCNN algorithm typically takes? Instead of running a CNN 2,000 times per image, we can run it just once per image and get all the regions of interest (regions containing some object).\n","\n","Ross Girshick, the author of RCNN, came up with the idea of running the CNN just once per image and then finding a way to share that computation across the 2,000 regions. In Fast RCNN, we feed the input image to the CNN, which in turn generates the convolutional feature maps. Using these maps, the regions of proposals are extracted. We then use an RoI pooling layer to reshape all the proposed regions into a fixed size, so that they can be fed into a fully connected network.\n","\n","Let’s break this down into steps to simplify the concept:\n","\n","- As with the earlier two techniques, we take an image as an input.\n","- This image is passed to a ConvNet which in turn generates the Regions of Interest.\n","- An RoI pooling layer is applied to all of these regions to reshape them as per the input of the ConvNet. Then, each region is passed on to a fully connected network.\n","- A softmax layer is used on top of the fully connected network to output classes. Along with the softmax layer, a linear regression layer is also used parallel to output bounding box coordinates for predicted classes.\n","\n","So, instead of using three different models (like in RCNN), Fast RCNN uses a single model which extracts features from the regions, divides them into different classes, and returns the boundary boxes for the identified classes simultaneously.\n","\n","To break this down even further, I’ll visualize each step to add a practical angle to the explanation.\n","\n","- We follow the now well-known step of taking an image as input:\n","\n","\n","![](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/fast-rcnn/image-10.png)\n","\n","- This image is passed to a ConvNet which returns the region of interests accordingly:\n","\n","![](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/fast-rcnn/image-11.png)\n","\n","- Then we apply the RoI pooling layer to the extracted regions of interest to make sure all the regions are of the same size:\n","\n","![](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/fast-rcnn/image-14.png)\n","\n","- Finally, these regions are passed on to a fully connected network which classifies them, as well as returns the bounding boxes using softmax and linear regression layers simultaneously:\n","\n","![](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/fast-rcnn/image-13.png)\n","\n","This is how Fast RCNN resolves two major issues of RCNN, i.e., passing one instead of 2,000 regions per image to the ConvNet, and using one instead of three different models for extracting features, classification, and generating bounding boxes.\n","\n","\n","## Problems with Fast RCNN\n","But even Fast RCNN has certain problem areas. It also uses selective search as a proposed method to find the Regions of Interest, which is a slow and time-consuming process. It takes around 2 seconds per image to detect objects, which is much better compared to RCNN. But when we consider large real-life datasets, then even a Fast RCNN doesn’t look so fast anymore.\n","\n","But there’s yet another object detection algorithm that trumps Fast RCNN. And something tells me you won’t be surprised by its name.\n","\n","## Paper\n","[Link](https://arxiv.org/pdf/1504.08083.pdf)"],"metadata":{"id":"eJyiusWA1PCl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCOL68W90r0A"},"outputs":[],"source":[]}]}