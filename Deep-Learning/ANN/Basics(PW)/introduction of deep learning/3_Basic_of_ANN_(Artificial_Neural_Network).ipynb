{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ8kZgmdJI5R"
      },
      "source": [
        "#Deep Learning With Computer Vision And Advanced NLP (DL_CV_NLP)\n",
        "\n",
        "$$ Revision Notes $$\n",
        "$$ A-Note-by-**Bappy Ahmed** $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWC8e6VKREO"
      },
      "source": [
        "# Basic Understanding of ANN (Artificial Neural Network)\n",
        "\n",
        "The solution to fitting more complex (*i.e.* non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply **add more weights**.\n",
        "\n",
        "There are two ways to add complexity:\n",
        "\n",
        "1. Add backward connections, so that output neurons feed back to input nodes, resulting in a **recurrent network**\n",
        "2. Add neurons between the input nodes and the outputs, creating an additional (\"hidden\") layer to the network, resulting in a **multi-layer perceptron**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Rayner_Alfred/publication/261206036/figure/fig3/AS:613952182571008@1523389015039/ANN-Architecture-Several-ANN-architectures-have-been-developed-such-as-feedforward-NN.png\" width=50%>\n",
        "\n",
        "How to train a multilayer network is not intuitive. Propagating the inputs forward over two layers is straightforward, since the outputs from the hidden layer can be used as inputs for the output layer. However, the process for updating the weights based on the prediction error is less clear, since it is difficult to know whether to change the weights on the input layer or on the hidden layer in order to improve the prediction.\n",
        "\n",
        "Updating a multi-layer perceptron (MLP) is a matter of: \n",
        "\n",
        "1. moving forward through the network, calculating outputs given inputs and current weight estimates\n",
        "2. moving backward updating weights according to the resulting error from forward propagation(using backpropagation method).\n",
        "\n",
        "In this sense, it is similar to a single-layer perceptron, except it has to be done twice, once for each layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jECeeRYFM0O6"
      },
      "source": [
        "##Activation Function of ANN:\n",
        "In ANN we use sigmoid as an activation function in each layer instead of step function.Because ANN can solve non-linear problem so the output can be varied. Sigmoid outputs numbers 0 to 1. On the other hand step function outputs just 0 or 1. \n",
        "\n",
        "<img src=\"https://nickmccullum.com/images/python-deep-learning/deep-learning-activation-functions/sigmoid-function.png\" width=\"400\" \n",
        "     height=\"300\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmO4OlVhPwqy"
      },
      "source": [
        "- Formula of Sigmoid:\n",
        "$$\\sigma (x)= \\frac {1}{1+e^{-x}}$$\n",
        "- It can take number between $-\\infty , + \\infty$ and gives output 0 to 1.\n",
        "\n",
        "### Need of Activation Function:\n",
        "- An activation function  added into an artificial neural network in order to help the network learn complex patterns in the data. \n",
        "- It also scales the data\n",
        "- It filters out the important portion of data\n",
        "- Without activation function Deep stack of network will behave like a single linear transformation.\n",
        "   - **Example:** without activation function\n",
        "\n",
        "   <img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/8.png?raw=true\" width=\"500\" \n",
        "     height=\"200\">\n",
        "\n",
        "     $z_1=x_1.w_1$,$z_2=w_2.z_1$ $$z_2=w_2.z_1$$\n",
        "     $$z_2=w_2.x_1w_1$$\n",
        "     $$z_2=Wx_1$$\n",
        "\n",
        "  - So, you can see it has been a single neuron.Behave like a single linear transformation.\n",
        "     \n",
        "     \n",
        "- Without activation function all the continuous function cannot be approximated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8PlKnwNR6Dh"
      },
      "source": [
        "## Weight update of ANN (Backpropagation intiution):\n",
        "\n",
        "* In the year 1986 a groundbreaking paper \"Learning Internal Representation by Error Propagation\" was published by -\n",
        "    * David Rumelhart,\n",
        "    * Geoffrey Hinton, &\n",
        "    * Ronald Williams \n",
        "    \n",
        "* It depicted an efficient way to update weights and biases of the network based on the error/loss function by passing twice through the network i.e forward and backward pass.\n",
        "    - forward pass: data is passed through the input layer to the hidden layer and it calculates ouput. Its nothing but making prediction.\n",
        "    - error calculation: Based on loss function error is calculated to check how much deviation is there from the ground truth or actual value and predicted value.\n",
        "    - error contribution from the each connection of the output layer is calculated.\n",
        "    - Then algo goes a layer deep and calculates how much previous layer contributed into the error of present layer and this way it propagates till the input layer.\n",
        "    - This reverse pass measures the error gradient accross all the connection.\n",
        "    - At last by using these error gradients a gradient step is performed to update the weights.\n",
        "    \n",
        "* In MLP key changes were to introduce a sigmoid activation function $$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6rMpfJjT8NQ"
      },
      "source": [
        "#Now lets get some intuition of ANN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifcOwXIMYEB1"
      },
      "source": [
        "<img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/1.png?raw=true\" width=\"500\" \n",
        "     height=\"300\">\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/2.png?raw=true\" width=\"500\" \n",
        "     height=\"300\">\n",
        "\n",
        "At the first layer is Input or Buffer layer. Second layer called hidden layer & the 3rd layer called output layer.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/3.png?raw=true\" width=\"500\" \n",
        "     height=\"300\">\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/4.png?raw=true\" width=\"500\" \n",
        "     height=\"300\">\n",
        "\n",
        "In the classification outputs neuron can be multiple but in the case of Regression output neuron might be one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqiOIihdhJ3J"
      },
      "source": [
        "## Simple Example:\n",
        "\n",
        "Lets take a simple neuron network , Here consider bias = 0\n",
        "\n",
        "<img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/5.png?raw=true\" width=\"500\" \n",
        "     height=\"300\">\n",
        "\n",
        "- So, \n",
        "$$z_1=(w_{11}.i_1)+(w_{12}.i_2)$$\n",
        "$$\\therefore \\hat{y_1} = act(z_1)$$\n",
        "$$z_2=(w_{21}.i_1)+(w_{22}.i_2)$$\n",
        "$$\\therefore\\hat{y_2} = act(z_2)$$\n",
        "\n",
        "\n",
        "Let's define it as a metrix form,\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "w_{11} & w_{12}\\\\ \n",
        "w_{21} & w_{22} \n",
        "\\end{bmatrix} * \\begin{bmatrix}i_1\\\\ i_2\\end{bmatrix}\n",
        "= \n",
        "\\begin{bmatrix}\n",
        "(w_{11}.i_1)+(w_{12}.i_2)\\\\ (w_{21}.i_1)+(w_{22}.i_2)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "z_1\\\\ z_2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        ">>\n",
        "\\begin{bmatrix}\n",
        "act(z_1)\\\\ act(z_2)\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\hat{y_1}\\\\ \\hat{y_2}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "$\n",
        "\\therefore W*X=Z\n",
        "$\n",
        "\n",
        "$act(Z)=\\hat{y}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy6wIlr7rH6T"
      },
      "source": [
        "## Error Calculation:\n",
        "<img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/6.png?raw=true\"\n",
        "width=\"500\" \n",
        "height=\"300\">\n",
        "\n",
        "Now weight will be updated based on the proportional error!\n",
        "\n",
        "<img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/7.png?raw=true\"\n",
        "width=\"500\" \n",
        "height=\"300\">\n",
        "\n",
        "In order to do that weight update rule (Backpropagration) can be used, that will be discuss further.\n",
        "\n",
        "- weight update rule:\n",
        "$$w= w-\\eta \\frac{\\partial e}{\\partial w}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOkAeyPbAsMD"
      },
      "source": [
        "# Difference between Perceptron & Neural Network:\n",
        "### Perceptron:\n",
        "- Perceptron is a single layer neural network, It can be also multi layer.\n",
        "- Perceptron is a linear classifier (binary).\n",
        "- It cann't solve non-linear problem.\n",
        "- Perceptron use step function as an activation function.\n",
        "\n",
        "###Neural Network:\n",
        "- A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.\n",
        "- A neuron network is consisted by single layer or multiple layer\n",
        "- It can be very depth\n",
        "- It can solve non-linear problems.\n",
        "- It can have many hidden layers.\n",
        "- It use sigmoid, ReLu, softmax etc. as an activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EzgkThXIxb0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}